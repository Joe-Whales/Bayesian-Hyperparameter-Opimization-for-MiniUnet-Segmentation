{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook\n",
    "from model import DiceLoss\n",
    "from train_utils import train_model, evaluate\n",
    "from utils import get_model\n",
    "import torch\n",
    "from hyperparameter_optimisation import (\n",
    "    perform_grid_search, \n",
    "    perform_bayesian_optimization,\n",
    "    plot_optimization_comparison,\n",
    "    plot_hyperparameter_importance\n",
    ")\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_space = {\n",
    "    # Model hyperparameters\n",
    "    'base_filters': [8, 16],\n",
    "    'depth': [2, 4],\n",
    "    'bilinear': [True],\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'learning_rate': [0.00005, 0.0001, 0.001],\n",
    "    'weight_decay': [1e-6, 1e-5, 1e-4],\n",
    "    'batch_size': [8, 16, 32],\n",
    "}\n",
    "\n",
    "# Define the Bayesian optimization parameter space\n",
    "bayesian_param_space = {\n",
    "    # Model hyperparameters\n",
    "    'base_filters': (8, 16),     # Will be rounded to integer\n",
    "    'depth': (2, 4),             # Will be rounded to integer\n",
    "    'bilinear': [True],   # Categorical\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'learning_rate': (0.00005, 0.005),  # Log scale\n",
    "    'weight_decay': (1e-6, 1e-4),      # Log scale\n",
    "    'batch_size': [8, 16, 32]          # Categorical\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 14:07:14,111] A new study created in memory with name: no-name-a661b4bb-9e46-4eae-8532-3ca5747f5323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian optimization with 35 trials\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196aa24e257f477ebb0b0402aad692a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 14:07:50,780] Trial 0 finished with value: 0.8752952103591258 and parameters: {'base_filters': 11, 'depth': 4, 'bilinear': True, 'learning_rate': 0.0014553179565665352, 'weight_decay': 1.575132049977973e-05, 'batch_size': 8}. Best is trial 0 with value: 0.8752952103591258.\n",
      "[I 2025-03-26 14:08:26,290] Trial 1 finished with value: 0.883694353942565 and parameters: {'base_filters': 15, 'depth': 3, 'bilinear': True, 'learning_rate': 0.0013035123791853842, 'weight_decay': 1.0994335574766194e-06, 'batch_size': 8}. Best is trial 1 with value: 0.883694353942565.\n",
      "[I 2025-03-26 14:09:00,493] Trial 2 finished with value: 0.7960381017519496 and parameters: {'base_filters': 9, 'depth': 2, 'bilinear': True, 'learning_rate': 0.00020298058052421552, 'weight_decay': 1.120760621186056e-05, 'batch_size': 32}. Best is trial 1 with value: 0.883694353942565.\n",
      "[I 2025-03-26 14:09:31,301] Trial 3 finished with value: 0.8170833170815908 and parameters: {'base_filters': 9, 'depth': 2, 'bilinear': True, 'learning_rate': 0.0002702051927323663, 'weight_decay': 8.168455894760166e-06, 'batch_size': 8}. Best is trial 1 with value: 0.883694353942565.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get dataset rather than dataloaders\n",
    "from dataset import get_datasets\n",
    "\n",
    "data_dir = 'square_segmentation_dataset/'\n",
    "\n",
    "# Get datasets\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(data_dir)\n",
    "\n",
    "# grid_results, grid_best = perform_grid_search(\n",
    "#     param_space=param_space,\n",
    "#     train_dataset=train_dataset,\n",
    "#     val_dataset=val_dataset,\n",
    "#     test_dataset=test_dataset,\n",
    "#     train_model_fn=train_model,\n",
    "#     get_model_fn=get_model,\n",
    "#     criterion_fn=DiceLoss,\n",
    "#     device=device,\n",
    "#     metric='iou_score',\n",
    "#     num_workers=8\n",
    "# )\n",
    "\n",
    "bayesian_results, bayesian_best, study = perform_bayesian_optimization(\n",
    "    param_space=bayesian_param_space,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    train_model_fn=train_model,\n",
    "    get_model_fn=get_model,\n",
    "    criterion_fn=DiceLoss,\n",
    "    device=device,\n",
    "    n_trials=35,\n",
    "    metric='iou_score',\n",
    "    seed=42,\n",
    "    num_workers=10\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nBest Grid Search Result:\")\n",
    "for k, v in grid_best.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nBest Bayesian Optimization Result:\")\n",
    "for k, v in bayesian_best.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize comparison\n",
    "plot_optimization_comparison(grid_results, bayesian_results, metric='iou_score')\n",
    "\n",
    "# Visualize hyperparameter importance from Bayesian optimization\n",
    "plot_hyperparameter_importance(bayesian_results, optimized_metric='iou_score')\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_config = bayesian_best if bayesian_best['iou_score'] > grid_best['iou_score'] else grid_best\n",
    "\n",
    "model_config = {\n",
    "    'base_filters': int(best_config['base_filters']),\n",
    "    'depth': int(best_config['depth']),\n",
    "    'bilinear': best_config['bilinear']\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    'learning_rate': best_config['learning_rate'],\n",
    "    'weight_decay': best_config['weight_decay'],\n",
    "    'batch_size': int(best_config['batch_size']),\n",
    "    'num_epochs': 100,  # Train longer for final model\n",
    "    'patience': 15      # More patience for final model\n",
    "}\n",
    "\n",
    "# Initialize model with best hyperparameters\n",
    "best_model = get_model(model_config, device=device)\n",
    "criterion = DiceLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    best_model.parameters(),\n",
    "    lr=train_config['learning_rate'],\n",
    "    weight_decay=train_config['weight_decay']\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "from dataset import get_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(data_dir,batch_size=train_config['batch_size'])\n",
    "\n",
    "# Train final model\n",
    "print(\"\\nTraining final model with best hyperparameters...\")\n",
    "best_model, history = train_model(\n",
    "    best_model, train_loader, val_loader, optimizer, criterion,\n",
    "    scheduler=scheduler, \n",
    "    num_epochs=train_config['num_epochs'],\n",
    "    patience=train_config['patience'],\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate(best_model, test_loader, criterion, device)\n",
    "print(\"\\nFinal model test set evaluation:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "from utils import save_model\n",
    "save_model(best_model, model_config, train_config, history, path=\"best_bayesian_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training and evaluation code (commented out for clarity)\n",
    "\n",
    "# from utils import visualize_samples\n",
    "# from dataset import get_dataloaders\n",
    "\n",
    "# # Initialize dataloaders\n",
    "# data_dir = dataset_config['dataset']['path']\n",
    "# train_loader, val_loader, test_loader = get_dataloaders(data_dir, batch_size=16, num_workers=6, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "# # Display dataset sizes\n",
    "# print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "# print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "# print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "\n",
    "# # Visualize training samples\n",
    "# # print(\"Training samples:\")\n",
    "# visualize_samples(train_loader)\n",
    "\n",
    "# from utils import get_model, test_model\n",
    "# model_config = {\n",
    "#     'base_filters': 16,\n",
    "#     'bilinear': True,\n",
    "#     'depth': 3\n",
    "# }\n",
    "\n",
    "# # Initialize and test model\n",
    "# model = get_model(model_config, device=device)\n",
    "# test_model(model, device=device)\n",
    "# from train_utils import train_model\n",
    "# from model import DiceLoss\n",
    "\n",
    "# train_config = {\n",
    "#     'learning_rate': 0.0005,\n",
    "#     'weight_decay': 1e-5,\n",
    "#     'batch_size': 16,\n",
    "#     'num_epochs': 50,\n",
    "#     'patience': 10\n",
    "# }\n",
    "\n",
    "# # Initialize optimizer, loss and scheduler\n",
    "# model = get_model(model_config, device=device)\n",
    "# criterion = DiceLoss()\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(),\n",
    "#     lr=train_config['learning_rate'],\n",
    "#     weight_decay=train_config['weight_decay']\n",
    "# )\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "# )\n",
    "\n",
    "# # Train model\n",
    "# print(\"Starting training...\")\n",
    "# model, history = train_model(\n",
    "#     model, train_loader, val_loader, optimizer, criterion,\n",
    "#     scheduler=scheduler, \n",
    "#     num_epochs=train_config['num_epochs'],\n",
    "#     patience=train_config['patience'],\n",
    "#     device=device\n",
    "# )\n",
    "# from utils import plot_history\n",
    "# plot_history(history)\n",
    "# from train_utils import evaluate \n",
    "# from utils import visualize_predictions\n",
    "\n",
    "# test_metrics = evaluate(model, test_loader, criterion, device)\n",
    "# print(\"Test set evaluation:\")\n",
    "# for metric, value in test_metrics.items():\n",
    "#     print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# # Visualize predictions\n",
    "# print(\"Predictions on test set:\")\n",
    "# visualize_predictions(model, test_loader, device=device)\n",
    "# from utils import save_model, load_model\n",
    "\n",
    "# # Save trained model\n",
    "# save_model(model, model_config, train_config, history)\n",
    "\n",
    "# print(\"Training and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stitch-o",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
